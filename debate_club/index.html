<!doctype html>
<html lang="en">



<head>
	<meta charset="UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
	<link rel="icon" type="image/svg+xml" href="favicon.svg" />
    <title>Teachable Machine</title>
    <link rel="stylesheet" href="style.css" />
</head>
<body>
<header>
	<div id="home-link">
        <a href="../">back to main page</a>
    </div>
	<h1>Debate Club</h1>
</header>


   <footer>
	<h2>Introduction</h2>
	<p>As part of our course, we also held a debate on the advantages and disadvantages of AI. Since we have worked a lot with AI this semester, I find it very meaningful to also discuss the use of AI on, for example, an ethical level. The given proposition was: The current trajectory of AI development, including the pursuit of AGI, presents more fundamental risks and challenges to human civilization than it offers opportunities and benefits.</p>
	<p>First, a brief definition of AGI:Artificial General Intelligence possesses intellectual abilities comparable to those of humans in all areas. It can optimize itself, communicate, plan ahead, and so on. Furthermore, it can independently surpass human capabilities and could therefore become humanity’s last invention.</p>
	<p>I was part of the team that argued for the proposition that AI causes more harm than good. These were our arguments:</p>
	<h2>Existential Risks</h2>
	<p>•	AI could pursue its own goals to the detriment of humanity </p>
	<p>•	Nick Bostrom (Oxford University): “Alignment Problem” → Competent but not “good.”</p>
	<p>•	Stuart Russell, Elon Musk, DeepMind, and OpenAI warn of existential threats.</p>
	<p>•	Example: Paperclip Maximizer.</p>

<h2>Lack of Regulation</h2>
	<p>•	The potential for power can be exploited by anyone without restrictions.</p>
	<p>•	EU AI Act: first steps, but not recognized or enforced worldwide.</p>
	<p>•	“Whistleblower reports”: developers receive warnings that are ignored.</p>
	<p>•	Example: AI learning to deceive humans.

<h2>Alignment Problem</h2>
	<p>•	Ensuring that AI always acts in humanity’s best interest is impossible.</p>
	<p>•	Stuart Russell (UC Berkeley): “…no method to build AGI safely.”</p>
	<p>•	Research in this area is still in its infancy and receives little funding.</p>
	<p>•	Example: hallucinations of LLMs.</p>

<h2>Economic Disruption</h2>
	<p>•	Will cost millions of jobs.</p>
	<p>•	Goldman Sachs (2023) estimated: “up to 300 million jobs worldwide” could be replaced.</p>
	<p>•	OECD: even highly qualified jobs such as legal analysis and software development are at risk.</p>
	<p>•	Example: ChatGPT and similar systems are already replacing illustrators and developers.

<h2>Concentration of Power</h2>
	<p>•	Larger models are controlled by big corporations.</p>
	<p>•	Digital power monopoly.</p>
	<p>•	Training, e.g., Gemini, requires billions, which smaller companies cannot afford.</p>
	<p>•	Data and computing power are centralized in “Big Tech,” leading to a “AI oligarchy.”</p>
	<p>•	Example: OpenAI was founded as a nonprofit, but now operates with little transparency.</p>

<h2>Misuse</h2>
	<p>•	AI can easily be used unethically: surveillance, propaganda, cyberattacks.</p>
	<p>•	China: facial recognition, social control, and monitoring of minorities.</p>
	<p>•	Deepfakes: manipulation of elections or smear campaigns through fabricated videos.</p>
	<p>•	Example: 2024 election campaign “Fake Obama” and the spread of misinformation.</p>

<h2>Conclusion</h2>
    <p>In conclusion, our team argued that AGI is far too poorly controlled. AGI harms society and deliberately exploits power. Moreover, the social system cannot keep pace with the speed of development. Prevention is more effective than disaster management, which is why we must take action now. Hubris is a sure path to failure; only responsible design can provide safety.</p>

   </footer>
</body>
</html>